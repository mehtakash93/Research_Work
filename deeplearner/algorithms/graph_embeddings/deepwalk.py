# ==============================================================================
# Name: deepwalk.py
# Author: Dakota Medd
# Version: 1.0
# Description: This file contains the deepwalk class, which manages code pertaining
# to the LINE algorithm.
# ==============================================================================

import sys
sys.path.append("../")

import numpy as np
import numpy.random as npr

import tensorflow as tf
import tensorflow_io as ti
import tensorflow_utils as tu

class Deepwalk(object):
    """
        Deepwalk is a graph embedding algorithm that attempts to find
        a lower dimensional representation of a graph by sampling a set of random
        walks of a certain length for each node, then running word2vec on the "sentences"

        This object works in conjunction with tensorflow (https://www.tensorflow.org/)
        to load and compute these embeddings. Tensorflow needs to be installed to run
        this algorithm.
    """
    def __init__(self):
        super(Deepwalk, self).__init__()
        self.pairs = []

    def __generate_random_walks(self,graph, length_of_path, number_per_node):
        """
            For each node in the graph object, this method will compute
            a number of random walks of a certain length.

            Parameters:
                graph: The networkx graph object
                length_of_path: The length of the random walk generated 
                    for each node
                number_per_node: The number of randoms walks generated 
                    for each node
            Returns:
                The list of generated random walks
        """
        random_walks = []
        for i in graph.nodes():
            for j in range(0,number_per_node):
                path = [i]

                for k in range(0,length_of_path):
                    sample_set = filter(lambda x: x not in path, graph.all_neighbors(i,i[-1]))
                    if len(sample_set) == 0:
                        break
                    sample = npr.choice(sample_set, size=1)
                    while(sample in path):
                        sample = npr.choice(sample_set, size=1)

                random_walks.append(path)

        return random_walks

    def __generate_sliding_windows(self, window_size, random_walks, length_of_path, index_encoding, vocab_size):
        """
            Takes a sliding window and samples the random_walks on the graph,
            generating a tuple with the first value being the index
            of the center node, and the other value being the
            union of the representations for the words around the center word

            For ease of computation, the window size must be odd. If it isn't,
            the window size will be increased by 1 to make it odd.

            Parameters:
                window_size: The size of the window for generating the sliding windows
                random_walks: The random walks taken from the graph
                length_of_path: The length of the random walks generated by the graph
                index_encoding: The indices of the nodes of the graph
                
        """
        if length_of_path < window_size:
            window_size = length_of_path

        if window_size % 2 == 1:
            window_size += 1

        pairs = []
        for r in random_walks:
            for i in range(0,len(r)-window_size+1):
                window = r[i:i+window_size]

                center_word = window[window_size/2+1]
                center_index  = index_encoding[center_word]

                compressed_index = []
                for w in window:
                    if w is not center_word:
                        compressed_index.append(index_encoding[w])

                pairs.append((center_index, compressed_index))

        self.pairs = pairs
        return pairs

    def load_graph_into_Deepwalk(self,graph, num_random_walks, length_of_random_walks, window_size, index_encoding, vocab_size):
        """
            Takes in a networkx Graph object, generates a set of
            random walks, takes a sliding window over the walks, then creates
            the input/output binary vectors expected by the DeepWalk algorithm

            Parameters:
                graph: A networkx graph object
                num_random_walks: The number of random walks to take for each node
                length_of_random_walks: The length of each random walk
            Return:
                A list of tuples, with the left element containing the one-hot encoding
                of the center word, and the right element containing the "or"ing of the
                one-hot encoding of the words surrounding the center word. One tuple is
                generated for each window
        """
        random_walks = self.__generate_random_walks(graph, length_of_random_walks, num_random_walks)
        pairs = self.__generate_sliding_windows(window_size, random_walks, length_of_random_walks, index_encoding, vocab_size)
        return pairs


    def tuple_generator(self,training_placeholder, test_placeholder):
        """
            Generator object that allows the class to feed the input/output pairs
            into the session.run instance inside the tensorflow pipeline
        """
        for i in self.pairs:
            yield {training_placeholder: i[0], test_placeholder: i[1]}

    def build_computationgraph(node_size, dim_size, num_sampled, train_inputs, train_labels, valid_dataset):
            """
                This method will build a tensorflow computation graph representing the auto-encoder used by DeepWalk 
                to learn embeddings. This function uses the default computation graph. It does not create a new
                computation graph, and therefore the default computation graph needs to be created before this
                function is ran inside it's with statement

                For more information, please read this:
                    https://www.tensorflow.org/versions/v0.6.0/tutorials/mnist/tf/index.html

                This code is a near exact copy from https://github.com/tensorflow/tensorflow/blob/0.6.0/tensorflow/examples/tutorials/word2vec/word2vec_basic.py
                Specifically, lines 152-170

                Parameters:
                    node_size: The number of nodes inside the graph
                    dim_size: The number of dimensions for the learned embedding
                    num_sampled: The number of edges to sample for negative-sampling (read paper)
                    train_inputs: The placeholder tensorflow node that represents the input node
                    train_labels: The placeholder tensorflow node that represents the output node
                    validation_dataset: The index vectors for node's whose neighor to node similarity
                        you want to check during the validation set
                Return:
                    The tensorflow node objects for:
                        the optimizer
                        loss function
                        normalized_embeddings
                        valid_embeddings
                        similarity computation

            """

            embeddings = tu.create_uweight(node_size, dim_size, "embedding")
            output_weights = tu.create_tnweight(node_size, dim_size, "context")
            bias = tu.create_bias(node_size, "context")

            # Look up embeddings for inputs.
            embed = tf.nn.embedding_lookup(embeddings, train_inputs)

            # Compute the average NCE loss for the batch.
            # tf.nce_loss automatically draws a new sample of the negative labels each
            # time we evaluate the loss.
            loss = tf.reduce_mean(
                tf.nn.nce_loss(output_weights, bias, embed, train_labels,
                            num_sampled, node_size))

            optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

            # Compute the cosine similarity between minibatch examples and all embeddings.
            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))

            normalized_embeddings = embeddings / norm

            # Get the embedding for the validation dataset
            valid_embeddings = tf.nn.embedding_lookup(
              normalized_embeddings, valid_dataset)

            # Compute the full similarity matrix
            similarity = tf.matmul(
              valid_embeddings, normalized_embeddings, transpose_b=True)
            return (optimizer,loss,normalized_embeddings,valid_embeddings,similarity)
print np.zeros(4)